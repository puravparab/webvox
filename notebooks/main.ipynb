{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4163a49d-c287-4082-a995-56df63673caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from utils import Content, login_hf, load_text_model\n",
    "login_hf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b57d4ce-fae5-4d31-a2cc-f9f023a608a6",
   "metadata": {},
   "source": [
    "# Webvox\n",
    "Get audio summaries of any website, blog or paper\n",
    "\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-View_on_GitHub-blue?logo=GitHub)](https://github.com/puravparab/webvox)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b006bbb7-7488-43ab-9eed-ffddd47ae40c",
   "metadata": {},
   "source": [
    "---\n",
    "## Table of Contents\n",
    "\n",
    "1. [Data](#1.-Data-)\n",
    "    - 1.1 [Blog](#1.1-Blog-)\n",
    "    - 1.2 [Website](#Website-)\n",
    "    - 1.3 [Paper](#1.2-Paper-)\n",
    "2. [Summarization](#2.-Summarization-)\n",
    "    - 2.1 [Llama 3.2B](#2.1-Llama-3.2-3B-Instruct-GGUF-)\n",
    "3. [Audio](#3.-Audio-)\n",
    "    - 3.1 [MelosTTS](#3.1-MelosTTS-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85525b68-120c-49cb-b082-9b4034f93b5e",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Data <a id='1.-Data-'></a>\n",
    "\n",
    "Let's import data from blogs, websites and papers that we can summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fcc19f-0146-4740-911a-662efb45a147",
   "metadata": {},
   "source": [
    "### 1.1 Blog <a id='1.1-Blog-'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8ed1bae-f843-4ffd-b387-b505e9c34940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 1582\n",
      "\n",
      "Blog content:\n",
      "'Founder Mode September 2024 At a YC event last week Brian Chesky gave a talk that everyone who\n",
      "was there will remember. Most founders I talked to afterward said\n",
      "it was the best they'd ever heard. Ron Conway, for the first time\n",
      "in his life, forgot to take notes. I'm not going to try to reproduce\n",
      "it here. Instead I want to talk about a question it raised. The theme of Brian's talk was that the conve'\n"
     ]
    }
   ],
   "source": [
    "# Insert url of a blog below\n",
    "url = \"https://paulgraham.com/foundermode.html\"\n",
    "\n",
    "blog = Content(url, 'blog')\n",
    "blog.scrape()\n",
    "print(f\"Token count: {blog.token_count}\")\n",
    "print(f\"\\nBlog content:\\n'{blog.text[:400]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c726e33-a43c-4880-be1a-edbd343fb65f",
   "metadata": {},
   "source": [
    "### 1.2 Paper <a id='1.2-Paper-'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8edc1c3-ab00-4da0-98d5-e6595567726c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 12519\n",
      "\n",
      "Blog content:\n",
      "'Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. 1 Introduction Recurrent neural networks, long short-term memory [ 13 ] and gated recurrent [ 7 ]'\n"
     ]
    }
   ],
   "source": [
    "# Insert url of a paper below\n",
    "url = \"https://ar5iv.labs.arxiv.org/html/1706.03762\"\n",
    "\n",
    "paper = Content(url, 'blog')\n",
    "paper.scrape()\n",
    "print(f\"Token count: {paper.token_count}\")\n",
    "print(f\"\\nBlog content:\\n'{paper.text[1754:3000]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be115e0a-5064-46db-ae47-db34f3d66a4b",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Summarization <a id='2.-Summarization-'></a>\n",
    "\n",
    "Using various LLMs to summarize content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b54fb7b-3ed8-446c-a257-a832e6a993b3",
   "metadata": {},
   "source": [
    "### 2.1 Llama-3.2-3B-Instruct-GGUF <a id='Llama-3.2-3B-Instruct-GGUF-'></a>\n",
    "We are using 4 bit quantized version of Llama 3.2B Instruct to summarize\n",
    "\n",
    "https://huggingface.co/lmstudio-community/Llama-3.2-3B-Instruct-GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3b64ac1-a397-4f02-abd3-a55fc6fc1681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing model: `Llama-3.2-3B-Instruct-Q4_K_M.gguf` from models/\n"
     ]
    }
   ],
   "source": [
    "# import model from hugging face\n",
    "llm = load_text_model(\n",
    "    repo_id=\"lmstudio-community/Llama-3.2-3B-Instruct-GGUF\",\n",
    "\tfilename=\"Llama-3.2-3B-Instruct-Q4_K_M.gguf\",\n",
    "    verbose=False,\n",
    "    context_length=15000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08470ad5-f05b-4892-88a8-a706a141ee0e",
   "metadata": {},
   "source": [
    "**Inference:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0a0db15-39e9-43fb-b43a-40ea2d4e57ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\"blog\": {}, \"paper\": {}} # Store summarization\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant that accurately summarizes content given to you.\n",
    "The summary must be in paragraph form that is easy to read out loud.\n",
    "Start the summary without any introductory sentence before it.\n",
    "\"\"\"\n",
    "user_prompt = \"\"\"Summarize the following content:\\n\\n\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641ee15c-0da1-49a9-ac9a-f1e9bc8cc5f9",
   "metadata": {},
   "source": [
    "#### Blog:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d225afa-ca16-468e-ab9d-cf1dabae622d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 12s, sys: 596 ms, total: 7min 13s\n",
      "Wall time: 48.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Blog\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": f\"{user_prompt}{blog.text[:]}\"}\n",
    "]\n",
    "output[\"blog\"] = llm.create_chat_completion(messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca9e0f54-5126-4909-afde-7c9ffa80fe02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Conventional wisdom on how to run larger companies is that founders should hire good people and give them room to do their jobs, but this approach has led to disastrous results for many. Instead, successful founders have had to figure out a better way on their own, often by studying how Steve Jobs ran Apple. \n",
      "\n",
      "A key realization is that there are two different ways to run a company: founder mode and manager mode. The conventional wisdom of hiring good people and giving them room to do their jobs is actually how to run a company like a manager, not a founder. \n",
      "\n",
      "This approach can lead to the \"gaslighting\" of founders, where they feel like they're being told they're mistaken when they're not, by both those giving them advice and those they're trying to advise. \n",
      "\n",
      "In contrast, founder mode involves engaging with the company directly and making decisions that benefit the company's growth, rather than treating employees as black boxes to be managed. This approach could involve skip-level meetings, where the founder meets directly with key stakeholders, and annual retreats to bring together a small group of high-priority employees.\n",
      "\n",
      "While it's clear that founder mode will require more delegation and flexibility than traditional management approaches, it also has the potential to work better for companies. As founders experiment with this new approach, they're already achieving positive results despite facing criticism and bad advice from others. \n",
      "\n",
      "However, there's still much to learn about founder mode, and it's essential to recognize that it won't be easy to implement and that it requires a unique approach to management.\n"
     ]
    }
   ],
   "source": [
    "blog_content = output['blog']['choices'][0]['message']['content']\n",
    "print(blog_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c0d475-075c-4ce9-871f-79c8fe7bfc65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c05168b-d511-4156-b3d1-0504afbd5264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 13min 34s, sys: 5.21 s, total: 1h 13min 39s\n",
      "Wall time: 7min 13s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# # Blog\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a helpful assistant that accurately summarizes content given to you.\"},\n",
    "#     {\"role\": \"user\", \"content\": f\"Summarize the following content:\\n\\n{paper.text[:]}\"}\n",
    "# ]\n",
    "# output[\"paper\"] = llm.create_chat_completion(messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ed7a78e-879c-4046-94cb-5cd06aee2f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The article presents the Transformer, a sequence transduction model based solely on self-attention mechanisms. It replaces traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs) with self-attention, which allows the model to parallelize computations and reduce training time.\n",
      "\n",
      "Here is a summary of the key points:\n",
      "\n",
      "1. **Transformer Architecture**: The Transformer consists of an encoder and a decoder, both of which use stacked self-attention and point-wise, fully connected layers.\n",
      "2. **Self-Attention Mechanism**: Self-attention allows the model to attend to different positions of the same sequence and compute a representation of the sequence.\n",
      "3. **Multi-Head Attention**: Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.\n",
      "4. **Advantages**: The Transformer has several advantages, including parallelization, reduced training time, and improved performance on machine translation tasks.\n",
      "5. **Training Data and Hardware**: The models were trained on the WMT 2014 English-German and WMT 2014 English-French datasets, with the big model achieving a BLEU score of 28.4 and 41.0, respectively.\n",
      "6. **Comparison with Other Models**: The Transformer outperforms previous state-of-the-art models, including ByteNet, Deep-Att, and ConvS2S, and achieves better results in both translation and constituency parsing tasks.\n",
      "7. **Conclusion**: The Transformer has demonstrated its effectiveness in sequence transduction tasks and has the potential to revolutionize the field of machine translation.\n",
      "\n",
      "The main contributions of this article are:\n",
      "\n",
      "* The introduction of the Transformer architecture, which replaces traditional RNNs and CNNs with self-attention mechanisms.\n",
      "* The demonstration of the Transformer's effectiveness in machine translation tasks, including parallelization and reduced training time.\n",
      "* The comparison of the Transformer with other state-of-the-art models and the achievement of better results in both translation and constituency parsing tasks.\n"
     ]
    }
   ],
   "source": [
    "# paper_content = output['paper']['choices'][0]['message']['content']\n",
    "# print(paper_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879a037a-41b3-4b7d-90fe-ba39f873d9a7",
   "metadata": {},
   "source": [
    "## 3. Audio <a id='3.-Audio-'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01547ae-f9b8-4ae6-a6e7-1bbcfdef1784",
   "metadata": {},
   "source": [
    "### 3.1 MelosTTS <a id='3.1-MelosTTS-'></a>\n",
    "\n",
    "We're using the MelosTTS text-to-speech model\n",
    "\n",
    "https://huggingface.co/myshell-ai/MeloTTS-English\n",
    "\n",
    "https://github.com/myshell-ai/MeloTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77be95c2-b4a7-4581-8bd3-161d7ecef5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
