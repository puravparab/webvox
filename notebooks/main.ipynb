{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4163a49d-c287-4082-a995-56df63673caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from utils import Content, login_hf, load_model\n",
    "login_hf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b57d4ce-fae5-4d31-a2cc-f9f023a608a6",
   "metadata": {},
   "source": [
    "# Webvox\n",
    "Get audio summaries of any website, blog or paper\n",
    "\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-View_on_GitHub-blue?logo=GitHub)](https://github.com/puravparab/webvox)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b006bbb7-7488-43ab-9eed-ffddd47ae40c",
   "metadata": {},
   "source": [
    "---\n",
    "## Table of Contents\n",
    "\n",
    "1. [Data](#1.-Data-)\n",
    "    - 1.1 [Blog](#1.1-Blog-)\n",
    "    - 1.2 [Website](#Website-)\n",
    "    - 1.3 [Paper](#1.2-Paper-)\n",
    "2. [Summarization](#2.-Summarization-)\n",
    "    - 2.1 [Llama 3.2B](#2.1-Llama-3.2-3B-Instruct-GGUF-)\n",
    "3. [Audio](#Audio-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85525b68-120c-49cb-b082-9b4034f93b5e",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Data <a id='1.-Data-'></a>\n",
    "\n",
    "Let's import data from blogs, websites and papers that we can summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fcc19f-0146-4740-911a-662efb45a147",
   "metadata": {},
   "source": [
    "### 1.1 Blog <a id='1.1-Blog-'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8ed1bae-f843-4ffd-b387-b505e9c34940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 1582\n",
      "\n",
      "Blog content:\n",
      "'Founder Mode September 2024 At a YC event last week Brian Chesky gave a talk that everyone who\n",
      "was there will remember. Most founders I talked to afterward said\n",
      "it was the best they'd ever heard. Ron Conway, for the first time\n",
      "in his life, forgot to take notes. I'm not going to try to reproduce\n",
      "it here. Instead I want to talk about a question it raised. The theme of Brian's talk was that the conve'\n"
     ]
    }
   ],
   "source": [
    "# Insert url of a blog below\n",
    "url = \"https://paulgraham.com/foundermode.html\"\n",
    "\n",
    "blog = Content(url, 'blog')\n",
    "blog.scrape()\n",
    "print(f\"Token count: {blog.token_count}\")\n",
    "print(f\"\\nBlog content:\\n'{blog.text[:400]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c726e33-a43c-4880-be1a-edbd343fb65f",
   "metadata": {},
   "source": [
    "### 1.2 Paper <a id='1.2-Paper-'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8edc1c3-ab00-4da0-98d5-e6595567726c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 12519\n",
      "\n",
      "Blog content:\n",
      "'Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. 1 Introduction Recurrent neural networks, long short-term memory [ 13 ] and gated recurrent [ 7 ]'\n"
     ]
    }
   ],
   "source": [
    "# Insert url of a paper below\n",
    "url = \"https://ar5iv.labs.arxiv.org/html/1706.03762\"\n",
    "\n",
    "paper = Content(url, 'blog')\n",
    "paper.scrape()\n",
    "print(f\"Token count: {paper.token_count}\")\n",
    "print(f\"\\nBlog content:\\n'{paper.text[1754:3000]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be115e0a-5064-46db-ae47-db34f3d66a4b",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Summarization <a id='2.-Summarization-'></a>\n",
    "\n",
    "Using various LLMs to summarize content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b54fb7b-3ed8-446c-a257-a832e6a993b3",
   "metadata": {},
   "source": [
    "### 2.1 Llama-3.2-3B-Instruct-GGUF <a id='Llama-3.2-3B-Instruct-GGUF-'></a>\n",
    "We are using 4 bit quantized version of Llama 3.2B Instruct to summarize\n",
    "\n",
    "https://huggingface.co/lmstudio-community/Llama-3.2-3B-Instruct-GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3b64ac1-a397-4f02-abd3-a55fc6fc1681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing model: `Llama-3.2-3B-Instruct-Q4_K_M.gguf` from models/\n"
     ]
    }
   ],
   "source": [
    "# import model from hugging face\n",
    "llm = load_model(\n",
    "    repo_id=\"lmstudio-community/Llama-3.2-3B-Instruct-GGUF\",\n",
    "\tfilename=\"Llama-3.2-3B-Instruct-Q4_K_M.gguf\",\n",
    "    verbose=False,\n",
    "    context_length=15000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08470ad5-f05b-4892-88a8-a706a141ee0e",
   "metadata": {},
   "source": [
    "**Inference:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0a0db15-39e9-43fb-b43a-40ea2d4e57ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\"blog\": {}, \"paper\": {}} # Store summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641ee15c-0da1-49a9-ac9a-f1e9bc8cc5f9",
   "metadata": {},
   "source": [
    "Blog:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d225afa-ca16-468e-ab9d-cf1dabae622d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 2s, sys: 847 ms, total: 8min 3s\n",
      "Wall time: 57.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Blog\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that accurately summarizes content given to you.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Summarize the following content:\\n\\n{blog.text[:]}\"}\n",
    "]\n",
    "output[\"blog\"] = llm.create_chat_completion(messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca9e0f54-5126-4909-afde-7c9ffa80fe02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The article discusses the concept of \"Founder Mode\" and its potential to revolutionize the way companies are run. The author was inspired by a recent talk by Brian Chesky, the founder of Airbnb, who shared his experiences of navigating the challenges of scaling a company and discovering a better way to run it. Chesky's approach, which emphasizes the importance of founders being engaged and hands-on in the company's operations, differs from the conventional wisdom of hiring professional managers and giving them room to operate.\n",
      "\n",
      "The author suggests that there are two different modes of running a company: \"founder mode\" and \"manager mode\". Founder mode is characterized by the CEO engaging directly with key stakeholders, including employees, customers, and partners, and making decisions that are driven by intuition and passion. This approach is in contrast to manager mode, which relies on hierarchical structures and compartmentalized decision-making.\n",
      "\n",
      "The article notes that the conventional wisdom about how to run companies is often flawed, and that many successful founders have reported struggling with the same advice that is now being offered as best practice. The author hypothesizes that the reason for this is that the advice is geared towards companies that are already large and professionalized, rather than startups or small companies where the CEO plays a more significant role.\n",
      "\n",
      "The article also touches on some potential benefits and challenges of founder mode, including the importance of delegating tasks and trusting employees, and the risk of founders becoming too involved in day-to-day operations. The author suggests that there is currently a lack of understanding about founder mode, and that it may take some time for this concept to be fully understood and developed.\n",
      "\n",
      "Finally, the author expresses some caution about the potential misuses of the founder mode concept, warning that founders who are unable to delegate effectively will use founder mode as an excuse, and that managers who are not founders may try to adopt founder-like behaviors. However, the author remains optimistic about the potential for founder mode to improve the way companies are run.\n"
     ]
    }
   ],
   "source": [
    "blog_content = output['blog']['choices'][0]['message']['content']\n",
    "print(blog_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c0d475-075c-4ce9-871f-79c8fe7bfc65",
   "metadata": {},
   "source": [
    "Paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c05168b-d511-4156-b3d1-0504afbd5264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 22min 13s, sys: 5.44 s, total: 1h 22min 18s\n",
      "Wall time: 8min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Blog\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that accurately summarizes content given to you.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Summarize the following content:\\n\\n{paper.text[:]}\"}\n",
    "]\n",
    "output[\"paper\"] = llm.create_chat_completion(messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ed7a78e-879c-4046-94cb-5cd06aee2f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The article presents the Transformer, a sequence transduction model based solely on self-attention mechanisms. It replaces traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs) with self-attention, which allows the model to parallelize computations and reduce training time.\n",
      "\n",
      "Here is a summary of the key points:\n",
      "\n",
      "1. **Transformer Architecture**: The Transformer consists of an encoder and a decoder, both of which use stacked self-attention and point-wise, fully connected layers.\n",
      "2. **Self-Attention Mechanism**: Self-attention allows the model to attend to different positions of the same sequence and compute a representation of the sequence.\n",
      "3. **Multi-Head Attention**: Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.\n",
      "4. **Advantages**: The Transformer has several advantages, including parallelization, reduced training time, and improved performance on machine translation tasks.\n",
      "5. **Training Data and Hardware**: The models were trained on the WMT 2014 English-German and WMT 2014 English-French datasets, with the big model achieving a BLEU score of 28.4 and 41.0, respectively.\n",
      "6. **Comparison with Other Models**: The Transformer outperforms previous state-of-the-art models, including ByteNet, Deep-Att, and ConvS2S, and achieves better results in both translation and constituency parsing tasks.\n",
      "7. **Conclusion**: The Transformer has demonstrated its effectiveness in sequence transduction tasks and has the potential to revolutionize the field of machine translation.\n",
      "\n",
      "The main contributions of this article are:\n",
      "\n",
      "* The introduction of the Transformer architecture, which replaces traditional RNNs and CNNs with self-attention mechanisms.\n",
      "* The demonstration of the Transformer's effectiveness in machine translation tasks, including parallelization and reduced training time.\n",
      "* The comparison of the Transformer with other state-of-the-art models and the achievement of better results in both translation and constituency parsing tasks.\n"
     ]
    }
   ],
   "source": [
    "paper_content = output['paper']['choices'][0]['message']['content']\n",
    "print(paper_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f95a9b-4716-4e20-a640-1fa91a921cba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
