{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4163a49d-c287-4082-a995-56df63673caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b57d4ce-fae5-4d31-a2cc-f9f023a608a6",
   "metadata": {},
   "source": [
    "# Webvox\n",
    "Get audio summaries of any website, blog or paper\n",
    "\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-View_on_GitHub-blue?logo=GitHub)](https://github.com/puravparab/webvox)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b006bbb7-7488-43ab-9eed-ffddd47ae40c",
   "metadata": {},
   "source": [
    "---\n",
    "## Table of Contents\n",
    "\n",
    "1. [Data](#Data-)\n",
    "    - 1.1. [Blog](#Blog-)\n",
    "    - 1.2. [Website](#Website-)\n",
    "    - 1.3. [Paper](#Paper-)\n",
    "2. [Summarization](#Summarization-)\n",
    "3. [Audio](#Audio-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85525b68-120c-49cb-b082-9b4034f93b5e",
   "metadata": {},
   "source": [
    "---\n",
    "## Data <a id='Data-'></a>\n",
    "\n",
    "Let's import data from blogs, websites and papers that we can summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d8c3a67-677f-4d44-8808-d183c6892d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraper import Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fcc19f-0146-4740-911a-662efb45a147",
   "metadata": {},
   "source": [
    "### Blog <a id='Blog-'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8ed1bae-f843-4ffd-b387-b505e9c34940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When To Do What You Love September 2024 There's some debate about whether it's a good idea to \"follow your\n",
      "passion.\" In fact the question is impossible to answer with a simple\n",
      "yes or no. Sometimes you should and sometimes you shouldn't, but\n",
      "the border between should and shouldn't is very complicated. The\n",
      "only way to give a general answer is to trace it. When people talk about this question, there's always an implicit\n",
      "\"instead of.\" All other things being equal, why wouldn't you work\n",
      "on what interests you the most? So even raising the question implies\n",
      "that all other things aren't equal, and that you have to choose\n",
      "between working on what interests you the most and something else,\n",
      "like what pays the best. And indeed if your main goal is to make money, you can't usually\n",
      "afford to work on what interests you the most. People pay you for\n",
      "doing what they want, not what you want. But there's an obvious\n",
      "exception: when you both want the same thing. For example, if you\n",
      "love football, and you're g\n"
     ]
    }
   ],
   "source": [
    "# Insert url of a blog below\n",
    "url = \"https://paulgraham.com/when.html\"\n",
    "\n",
    "blog = Content(url, 'blog')\n",
    "blog.scrape()\n",
    "if blog.text:\n",
    "    print(blog.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c726e33-a43c-4880-be1a-edbd343fb65f",
   "metadata": {},
   "source": [
    "### Paper <a id='Paper-'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8edc1c3-ab00-4da0-98d5-e6595567726c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. 1 Introduction Recurrent neural networks, long short-term memory [ 13 ] and gated recurrent [ 7 ]\n"
     ]
    }
   ],
   "source": [
    "# Insert url of a paper below\n",
    "url = \"https://ar5iv.labs.arxiv.org/html/1706.03762\"\n",
    "\n",
    "blog = Content(url, 'blog')\n",
    "blog.scrape()\n",
    "if blog.text:\n",
    "    print(blog.text[1754:3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be115e0a-5064-46db-ae47-db34f3d66a4b",
   "metadata": {},
   "source": [
    "## Summarization <a id='Summarization-'></a>\n",
    "\n",
    "Using an LLM to summarize content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
