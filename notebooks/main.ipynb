{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4163a49d-c287-4082-a995-56df63673caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from utils import Content, login_hf, load_model\n",
    "login_hf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b57d4ce-fae5-4d31-a2cc-f9f023a608a6",
   "metadata": {},
   "source": [
    "# Webvox\n",
    "Get audio summaries of any website, blog or paper\n",
    "\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-View_on_GitHub-blue?logo=GitHub)](https://github.com/puravparab/webvox)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b006bbb7-7488-43ab-9eed-ffddd47ae40c",
   "metadata": {},
   "source": [
    "---\n",
    "## Table of Contents\n",
    "\n",
    "1. [Data](#1.-Data-)\n",
    "    - 1.1 [Blog](#1.1-Blog-)\n",
    "    - 1.2 [Website](#Website-)\n",
    "    - 1.3 [Paper](#1.2-Paper-)\n",
    "2. [Summarization](#2.-Summarization-)\n",
    "    - 2.1 [Llama 3.2B](#2.1-Llama-3.2-3B-Instruct-GGUF-)\n",
    "3. [Audio](#3.-Audio-)\n",
    "    - 3.1 [MelosTTS](#3.1-MelosTTS-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85525b68-120c-49cb-b082-9b4034f93b5e",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Data <a id='1.-Data-'></a>\n",
    "\n",
    "Let's import data from blogs, websites and papers that we can summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fcc19f-0146-4740-911a-662efb45a147",
   "metadata": {},
   "source": [
    "### 1.1 Blog <a id='1.1-Blog-'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8ed1bae-f843-4ffd-b387-b505e9c34940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 1582\n",
      "\n",
      "Blog content:\n",
      "'Founder Mode September 2024 At a YC event last week Brian Chesky gave a talk that everyone who\n",
      "was there will remember. Most founders I talked to afterward said\n",
      "it was the best they'd ever heard. Ron Conway, for the first time\n",
      "in his life, forgot to take notes. I'm not going to try to reproduce\n",
      "it here. Instead I want to talk about a question it raised. The theme of Brian's talk was that the conve'\n"
     ]
    }
   ],
   "source": [
    "# Insert url of a blog below\n",
    "url = \"https://paulgraham.com/foundermode.html\"\n",
    "\n",
    "blog = Content(url, 'blog')\n",
    "blog.scrape()\n",
    "print(f\"Token count: {blog.token_count}\")\n",
    "print(f\"\\nBlog content:\\n'{blog.text[:400]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c726e33-a43c-4880-be1a-edbd343fb65f",
   "metadata": {},
   "source": [
    "### 1.2 Paper <a id='1.2-Paper-'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8edc1c3-ab00-4da0-98d5-e6595567726c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 12519\n",
      "\n",
      "Blog content:\n",
      "'Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. 1 Introduction Recurrent neural networks, long short-term memory [ 13 ] and gated recurrent [ 7 ]'\n"
     ]
    }
   ],
   "source": [
    "# Insert url of a paper below\n",
    "url = \"https://ar5iv.labs.arxiv.org/html/1706.03762\"\n",
    "\n",
    "paper = Content(url, 'blog')\n",
    "paper.scrape()\n",
    "print(f\"Token count: {paper.token_count}\")\n",
    "print(f\"\\nBlog content:\\n'{paper.text[1754:3000]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be115e0a-5064-46db-ae47-db34f3d66a4b",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Summarization <a id='2.-Summarization-'></a>\n",
    "\n",
    "Using various LLMs to summarize content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b54fb7b-3ed8-446c-a257-a832e6a993b3",
   "metadata": {},
   "source": [
    "### 2.1 Llama-3.2-3B-Instruct-GGUF <a id='Llama-3.2-3B-Instruct-GGUF-'></a>\n",
    "We are using 4 bit quantized version of Llama 3.2B Instruct to summarize\n",
    "\n",
    "https://huggingface.co/lmstudio-community/Llama-3.2-3B-Instruct-GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3b64ac1-a397-4f02-abd3-a55fc6fc1681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing model: `Llama-3.2-3B-Instruct-Q4_K_M.gguf` from models/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33345ab1020643ae9d6c57d7f4a1fa81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Llama-3.2-3B-Instruct-Q4_K_M.gguf:   0%|          | 0.00/2.02G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import model from hugging face\n",
    "llm = load_text_model(\n",
    "    repo_id=\"lmstudio-community/Llama-3.2-3B-Instruct-GGUF\",\n",
    "\tfilename=\"Llama-3.2-3B-Instruct-Q4_K_M.gguf\",\n",
    "    verbose=False,\n",
    "    context_length=15000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08470ad5-f05b-4892-88a8-a706a141ee0e",
   "metadata": {},
   "source": [
    "**Inference:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0a0db15-39e9-43fb-b43a-40ea2d4e57ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\"blog\": {}, \"paper\": {}} # Store summarization\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant that accurately summarizes content given to you.\n",
    "The summary must be in paragraph form that is easy to read out loud.\n",
    "Start the summary without any introductory sentence before it.\n",
    "\"\"\"\n",
    "user_prompt = \"\"\"Summarize the following content:\\n\\n\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641ee15c-0da1-49a9-ac9a-f1e9bc8cc5f9",
   "metadata": {},
   "source": [
    "#### Blog:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d225afa-ca16-468e-ab9d-cf1dabae622d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 20s, sys: 301 ms, total: 7min 20s\n",
      "Wall time: 50.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Blog\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": f\"{user_prompt}{blog.text[:]}\"}\n",
    "]\n",
    "output[\"blog\"] = llm.create_chat_completion(messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca9e0f54-5126-4909-afde-7c9ffa80fe02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The conventional wisdom on how to run larger companies is that founders need to hire good people and give them room to do their jobs, but this approach can lead to disastrous results. Instead, a more effective approach is to adopt a \"founder mode\" that allows for closer involvement and decision-making. This approach has been successful in companies like Airbnb, which has implemented a unique management structure that is tailored to its founder's style. The idea behind founder mode is to create a management system that is more adaptable and responsive to the needs of the company, rather than relying on a rigid hierarchical structure. This approach involves skipping-level meetings, where the CEO interacts directly with key stakeholders rather than only through their direct reports, and a more modular approach to decision-making. \n",
      "\n",
      "A key aspect of founder mode is that it breaks the principle that the CEO should only engage with the company through their direct reports. This approach allows for greater autonomy and trust in managers, which can lead to a more effective and efficient management system. However, it also means that the CEO needs to be willing to get involved in the details of the company's operations, which can be challenging for some leaders. \n",
      "\n",
      "The concept of founder mode has been identified through the experiences of individual founders who have had to adapt and innovate in response to bad advice and conventional wisdom. These founders have found that a more hands-on approach, tailored to their company's specific needs, is more effective in achieving success. The idea of founder mode is still in its early stages, and there is much to be learned and explored in this approach. As the concept becomes more established, it will be interesting to see how it is applied and whether it leads to new and innovative solutions for managing large companies. \n",
      "\n",
      "The author of this article is optimistic that founder mode has the potential to revolutionize the way companies are managed, but also acknowledges that there are potential pitfalls to be aware of, such as the misuse of the concept by leaders who are not suited to this approach. Nevertheless, the potential benefits of founder mode are significant, and it is likely that this approach will continue to be explored and refined in the years to come.\n"
     ]
    }
   ],
   "source": [
    "blog_content = output['blog']['choices'][0]['message']['content']\n",
    "print(blog_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c0d475-075c-4ce9-871f-79c8fe7bfc65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c05168b-d511-4156-b3d1-0504afbd5264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 13min 34s, sys: 5.21 s, total: 1h 13min 39s\n",
      "Wall time: 7min 13s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# # Blog\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a helpful assistant that accurately summarizes content given to you.\"},\n",
    "#     {\"role\": \"user\", \"content\": f\"Summarize the following content:\\n\\n{paper.text[:]}\"}\n",
    "# ]\n",
    "# output[\"paper\"] = llm.create_chat_completion(messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ed7a78e-879c-4046-94cb-5cd06aee2f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The article presents the Transformer, a sequence transduction model based solely on self-attention mechanisms. It replaces traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs) with self-attention, which allows the model to parallelize computations and reduce training time.\n",
      "\n",
      "Here is a summary of the key points:\n",
      "\n",
      "1. **Transformer Architecture**: The Transformer consists of an encoder and a decoder, both of which use stacked self-attention and point-wise, fully connected layers.\n",
      "2. **Self-Attention Mechanism**: Self-attention allows the model to attend to different positions of the same sequence and compute a representation of the sequence.\n",
      "3. **Multi-Head Attention**: Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.\n",
      "4. **Advantages**: The Transformer has several advantages, including parallelization, reduced training time, and improved performance on machine translation tasks.\n",
      "5. **Training Data and Hardware**: The models were trained on the WMT 2014 English-German and WMT 2014 English-French datasets, with the big model achieving a BLEU score of 28.4 and 41.0, respectively.\n",
      "6. **Comparison with Other Models**: The Transformer outperforms previous state-of-the-art models, including ByteNet, Deep-Att, and ConvS2S, and achieves better results in both translation and constituency parsing tasks.\n",
      "7. **Conclusion**: The Transformer has demonstrated its effectiveness in sequence transduction tasks and has the potential to revolutionize the field of machine translation.\n",
      "\n",
      "The main contributions of this article are:\n",
      "\n",
      "* The introduction of the Transformer architecture, which replaces traditional RNNs and CNNs with self-attention mechanisms.\n",
      "* The demonstration of the Transformer's effectiveness in machine translation tasks, including parallelization and reduced training time.\n",
      "* The comparison of the Transformer with other state-of-the-art models and the achievement of better results in both translation and constituency parsing tasks.\n"
     ]
    }
   ],
   "source": [
    "# paper_content = output['paper']['choices'][0]['message']['content']\n",
    "# print(paper_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879a037a-41b3-4b7d-90fe-ba39f873d9a7",
   "metadata": {},
   "source": [
    "## 3. Audio <a id='3.-Audio-'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01547ae-f9b8-4ae6-a6e7-1bbcfdef1784",
   "metadata": {},
   "source": [
    "### 3.1 MelosTTS <a id='3.1-MelosTTS-'></a>\n",
    "\n",
    "We're using the MelosTTS text-to-speech model\n",
    "\n",
    "https://huggingface.co/myshell-ai/MeloTTS-English\n",
    "\n",
    "https://github.com/myshell-ai/MeloTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77be95c2-b4a7-4581-8bd3-161d7ecef5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
